name: Full CI/CD Pipeline

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

env:
  AWS_REGION: ca-central-1
  ECR_REPO: spark-etl
  IMAGE_TAG: latest


jobs:
  # ──────────────────────────────────────────────
  # Build / Lint / Test / Push Docker Image
  # ──────────────────────────────────────────────
  build:
    name: Build & Push Spark Image
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Cache pip
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('data-pipeline/requirements.txt') }}

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          pip install -r data-pipeline/requirements.txt

      - name: Lint with Ruff
        run: ruff check .

      - name: Run tests
        run: |
          pytest --junitxml=pytest-results.xml

      - name: Upload test report
        uses: actions/upload-artifact@v4
        with:
          name: pytest-results
          path: pytest-results.xml

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Login to Amazon ECR
        uses: aws-actions/amazon-ecr-login@v1
        with :
          mask-password: true


      - name: Build and Push Docker image
        run: |
          docker buildx build \
            --platform linux/amd64 \
            -t ${{ secrets.AWS_ACCOUNT_ID }}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com/$ECR_REPO:$IMAGE_TAG \
            -f data-pipeline/docker/Dockerfile \
            --push \
            data-pipeline

  # ──────────────────────────────────────────────
  # Sync Airflow DAGs to MWAA S3 Bucket
  # ──────────────────────────────────────────────
  deploy-dags:
    name: Deploy DAGs to MWAA
    runs-on: ubuntu-latest
    needs: build
    steps:
      - uses: actions/checkout@v3

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Sync DAGs to S3
        run: |
          aws s3 sync data-pipeline/dags s3://${{ secrets.MWAA_DAGS_BUCKET }}/dags \
            --delete


  # ──────────────────────────────────────────────
  # Deploy Spark ETL Job to EKS
  # ──────────────────────────────────────────────
  deploy-etl:
    name: Deploy Spark ETL to EKS
    runs-on: ubuntu-latest
    needs: infra
    steps:
      - uses: actions/checkout@v3

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Install kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: '1.30.0'

      - name: Update kubeconfig
        run: |
          aws eks update-kubeconfig \
            --name ${{ secrets.EKS_CLUSTER_NAME }} \
            --region ${{ env.AWS_REGION }}

      - name: Render & apply Spark Job manifest
        run: |
          export ECR_URI=${{ secrets.AWS_ACCOUNT_ID }}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com/${{ env.ECR_REPO }}:${{ env.IMAGE_TAG }}
          envsubst < data-pipeline/k8s/spark-job.yaml | kubectl apply -f -

