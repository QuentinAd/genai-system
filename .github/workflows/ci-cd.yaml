name: Full CI/CD Pipeline

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

env:
  AWS_REGION: ca-central-1
  ECR_REPO: spark-etl
  IMAGE_TAG: latest

jobs:
  # ──────────────────────────────────────────────
  # 1) Build / Lint / Test / Push Docker Image
  # ──────────────────────────────────────────────
  build:
    name: Build & Push Spark Image
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Cache pip
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('data-pipeline/requirements.txt') }}

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          pip install -r data-pipeline/requirements.txt

      - name: Lint with Ruff
        run: ruff check .

      - name: Run tests
        run: |
          pytest --junitxml=pytest-results.xml

      - name: Upload test report
        uses: actions/upload-artifact@v4
        with:
          name: pytest-results
          path: pytest-results.xml

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Login to Amazon ECR
        uses: aws-actions/amazon-ecr-login@v1

      - name: Build and Push Docker image
        run: |
          docker buildx build \
            --platform linux/amd64 \
            -t $ECR_REPO:$IMAGE_TAG \
            -f data-pipeline/docker/Dockerfile \
            --push  \
            ${{ secrets.AWS_ACCOUNT_ID }}.dkr.ecr.$AWS_REGION.amazonaws.com/$ECR_REPO:$IMAGE_TAG

  # ──────────────────────────────────────────────
  # 2) Sync Airflow DAGs to MWAA S3 Bucket
  # ──────────────────────────────────────────────
  deploy-dags:
    name: Deploy DAGs to MWAA
    runs-on: ubuntu-latest
    needs: build
    steps:
      - uses: actions/checkout@v3

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Sync DAGs to S3
        run: |
          aws s3 sync data-pipeline/dags s3://${{ secrets.MWAA_DAGS_BUCKET }}/dags \
            --delete

  # ──────────────────────────────────────────────
  # 3) Terraform: Plan & Apply Infra
  # ──────────────────────────────────────────────
  infra:
    name: Terraform Apply
    runs-on: ubuntu-latest
    needs: deploy-dags
    defaults:
      run:
        working-directory: infra
    steps:
      - uses: actions/checkout@v3

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: 1.5.6

      - name: Terraform Init
        run: terraform init -input=false

      - name: Terraform Validate & Plan
        run: |
          terraform validate
          terraform plan -out=tfplan

      - name: Terraform Apply
        if: github.ref == 'refs/heads/main' && github.event_name == 'push'
        run: terraform apply -input=false tfplan

  # ──────────────────────────────────────────────
  # 4) Deploy Spark ETL Job to EKS
  # ──────────────────────────────────────────────
  deploy-etl:
    name: Deploy Spark ETL to EKS
    runs-on: ubuntu-latest
    needs: infra
    steps:
      - uses: actions/checkout@v3

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Install kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: '1.30.0'

      - name: Update kubeconfig
        run: |
          aws eks update-kubeconfig \
            --name ${{ secrets.EKS_CLUSTER_NAME }} \
            --region ${{ env.AWS_REGION }}

      - name: Apply Spark Job manifest
        run: |
          kubectl apply -f data-pipeline/k8s/spark-job.yaml
          kubectl rollout status job/spark-etl

